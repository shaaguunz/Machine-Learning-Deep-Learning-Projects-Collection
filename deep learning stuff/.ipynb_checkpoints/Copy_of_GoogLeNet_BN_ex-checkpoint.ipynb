{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs3Vxo5dmP7j"
   },
   "source": [
    "## Image classification GoogLeNet\n",
    "\n",
    "***NOTE***\n",
    "\n",
    "Be sure to use hardware acceleration to use the GPU. Click on `Runtime`, change `runtime type`, and select `GPU` for the *hardware accelerator* option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FUHcE89LT3x"
   },
   "source": [
    "* Add batch nomalisation to GoogLeNet\n",
    "* Remove the preprocessing which was applied in the ```resize_images``` function which was responsible to for the normalisation, and instead, add a batch normalisation layer before the first convolutional layer in the network. You will run into problems calling the ```summary()``` function depending on where you add batch norm.\n",
    "* How does the performance compare in both cases versus GoogLeNet without batch normalisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpvo-bqTmP7m"
   },
   "source": [
    "## Imports first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvCXvXq5mP7p"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56fx-uBSR070"
   },
   "source": [
    "# Module 1\n",
    "The first module uses a 64-channel 7x7 convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4qzSC_TB4U5"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 2D conv\n",
    "model.add(tf.keras.layers.Conv2D(64, 7, strides=2, padding='same',\n",
    "                                   activation='relu', input_shape = (96,96,1)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "# Max pooling\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2,\n",
    "                                      padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxFQlI9aSrMV"
   },
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLpaJcBQSqxx"
   },
   "outputs": [],
   "source": [
    "# 1x1 conv\n",
    "model.add(tf.keras.layers.Conv2D(64, 1, activation='relu'))\n",
    "\n",
    "# 2D 3x3 conv\n",
    "model.add(tf.keras.layers.Conv2D(192, 3, padding='same', activation='relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgbJzOtrS2GP"
   },
   "outputs": [],
   "source": [
    "class Inception(tf.keras.Model):\n",
    "    # `c1`--`c4` are the number of output channels for each branch\n",
    "    def __init__(self, c1, c2, c3, c4):\n",
    "        super().__init__()\n",
    "\n",
    "        # TO DO\n",
    "\n",
    "    def call(self, x):\n",
    "      \n",
    "        # TO DO\n",
    "\n",
    "        # Concatenate\n",
    "        return tf.keras.layers.Concatenate()([b1, b2, b3, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gfqiuHHS2It"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(64, (96, 128), (16, 32), 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHNvd9UXWJMh"
   },
   "source": [
    "The number of output channels of the second Inception block is increased to 128+192+96+64=480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mTO2-pbSq29"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(128, (128, 192), (32, 96), 64))\n",
    "\n",
    "# Remember based on the diagram above, a pooling layer is applied after each module\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKle16IdWXoV"
   },
   "source": [
    "# Module 4\n",
    "The fourth module is more complicated. It connects five Inception blocks in series\n",
    "\n",
    "* block 1 outputs: 192+208+48+64 = 512\n",
    "\n",
    "* block 2 outputs: 160+224+64+64 = 512\n",
    "\n",
    "* block 3 outputs: 128+526+64+64 = 512\n",
    "\n",
    "* block 3 outputs: 112+228+64+64 = 528\n",
    "\n",
    "* block 5 outputs: 256+320+128+128 = 832\n",
    "\n",
    "Each block is learning a different number of features due to usual reasons but also due to difference in number of channels from 1x1 conv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTtB4f9jRKo7"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(192, (96, 208), (16, 48), 64))\n",
    "model.add(Inception(160, (112, 224), (24, 64), 64))\n",
    "model.add(Inception(128, (128, 256), (24, 64), 64))\n",
    "model.add(Inception(112, (144, 288), (32, 64), 64))\n",
    "model.add(Inception(256, (160, 320), (32, 128), 128))\n",
    "\n",
    "# Remember based on the diagram above, a pooling layer is applied after each module\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsex3G6fX7ps"
   },
   "source": [
    "# Module 5\n",
    "\n",
    "Two inception blocks\n",
    "\n",
    "* block 1 output: 256+320+128+128=832\n",
    "\n",
    "* block 2 output: 384+384+128+128=1024\n",
    "\n",
    "* This block uses the global average pooling layer to change the height and width of each channel to 1, just as in NiN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h3DNre8W5Gy"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(256, (160, 320), (32, 128), 128))\n",
    "model.add(Inception(384, (192, 384), (48, 128), 128))\n",
    "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0hnnNudY23M"
   },
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDGsJOyqW5PQ"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvLNnz0vDnHc"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1GTR35cmP7v"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ_KjFJomP7x"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z51sdOntmP78"
   },
   "source": [
    "## Find the unique numbers from the train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o29O_gUTmP79"
   },
   "outputs": [],
   "source": [
    "classes = np.unique(Y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA_mGK_NVHwn"
   },
   "source": [
    "## Reshape needed\n",
    "\n",
    "Keras wants to know the depth of an image. \n",
    "\n",
    "For CNNS, Keras wants the format of the data as follows: [batches, width, height, depth]. \n",
    "\n",
    "In this case the colour channel/depth of the images is 1. Currently the shape is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a55ZpEbCB6kQ"
   },
   "source": [
    "But this doesn't have a depth value. So we can reshape it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8HTp2YHVH4_"
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfClFp-6K7P6"
   },
   "source": [
    "## Convert from categorical labels to one-hot encoded vectors\n",
    "\n",
    "In this case there are 10 classes so we can tell the function to convert into a vector of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3xjsPnVmP8j"
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7LVPnt1QPQM"
   },
   "source": [
    "## Small twist!\n",
    "\n",
    "API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQNnZHhNSEKW"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHvmE76ySEMx"
   },
   "outputs": [],
   "source": [
    "def resize_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    image = tf.image.resize(image, (96,96))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqEzyUPLSOdv"
   },
   "outputs": [],
   "source": [
    "train_ds = (train_ds\n",
    "                  .map(resize_images)\n",
    "                  .shuffle(buffer_size=10000)\n",
    "                  .batch(batch_size=64, drop_remainder=True))\n",
    "test_ds = (test_ds\n",
    "                  .map(resize_images)\n",
    "                  .batch(batch_size=32, drop_remainder=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gozmsPxhmP8v"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifPs0mftmP84"
   },
   "source": [
    "## Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8RZc1rhmP84"
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=2, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j9LecHgmP8_"
   },
   "source": [
    "## Predict on all the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TtfkDWjmP8_"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_zM7W80cFhJ"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0abcDBFmcCz5"
   },
   "outputs": [],
   "source": [
    "correct_values = np.argmax(Y_test,axis=-1)\n",
    "predicted_classes = np.argmax(predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7keRohxmP9H"
   },
   "outputs": [],
   "source": [
    "accuracy_score(predicted_classes,correct_values)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1Ik7Odwj9L6"
   },
   "source": [
    "More efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agkf9IKkiotI"
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "for x,y in test_ds.as_numpy_iterator():\n",
    "  targets.extend(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5n-nlHvkjhkL"
   },
   "outputs": [],
   "source": [
    "np.asarray(targets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnlUWwWmirtl"
   },
   "outputs": [],
   "source": [
    "accuracy_score(predicted_classes,np.argmax(targets,axis=-1))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pci1FMrJcuSs"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lPFztVMj3d6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
