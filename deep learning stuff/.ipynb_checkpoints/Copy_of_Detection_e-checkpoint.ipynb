{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAgqh3jy52qA"
   },
   "source": [
    "# Object localisation and detection\n",
    "\n",
    "* Classification is about determining whether something is in a picture. Does this image contain a cat, a dog or a bird?\n",
    "\n",
    "\n",
    "* Classifiaction and localisation is about determining whether something is in a picture, but, additionally finding where in the image the object is. Here, the goal is to draw a bounding box around the object. Typically, there is only one object in the image.\n",
    "\n",
    "* Detection, on the other hand, is about finding the location and classification of multiple entities within an image. Typically, there are multiple objects (from different classes) in the image you are trying to find.\n",
    "\n",
    "Here is a very famous image:\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*Hz6t-tokG1niaUfmcysusw.jpeg)\n",
    "\n",
    "* **Question**: In which real world application might you expect to see each of the first three types of problems shown above in the picture.\n",
    "\n",
    "* From what we saw before, you would typically follow this type of approach. You would have an input image which you would pass through a convolutional neural network and then you would have a softmax output layer with the same number of units as classes. You might have an additional class called \"background\" which denotes that there is an absense the objects you are interested in. *See image below.*\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1VH0apHRaQz8smuxzp4x9noi8N3Ee7gj8)\n",
    "\n",
    "* **Question** How would you suggest that we modify the network such that we can locate the objects were are interested in?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "What we can do is to add four extra network units that denote the midpoint of the object along with the height and width. *See image below.*\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1AUgQ1ee5xeQlQD-OZrvVhvAccls_4Fb9)\n",
    "\n",
    "* **Question:** what does this mean in terms of our training data? How will it have to be modified? What do you think the values of $b_x, b_y, b_w, b_h$ should be in this example, roughly?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* **Question:** How exactly can we define the targets y?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "1. We can define a variable $p_c$ as the presence of an object, and set it to 1 if there is an object, and set it to 0 otherwise. So if the image is class 4, then $p_c = 0$, and if the image class is say class 3, then $p_c = 1$, similarly for classes 1 and 2.\n",
    "\n",
    "2. We need to also have values of $b_x, b_y, b_w, b_h$ in y.\n",
    "\n",
    "3. We need to also have the one-hot encoded vector, as before, so let us call this $c_1, c_2, c_3$.\n",
    "\n",
    "*See image below.*\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=15nHvq_MnJNX72XMoOxodNQrP62UiWjqr)\n",
    "\n",
    "* **Question:** Can you guess how we will encode the target, y, for each of the classes. Start with pedestrian, car, motocycle and background.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*See image below.*\n",
    "![](https://drive.google.com/uc?export=view&id=1I7Zg_JjcHfzKHOlihi-Pw3rzWhjEh3dG)\n",
    "\n",
    "In the case of $p_c = 0$, the $?$ marks denote \"don't care\" as this is not relevant when there is no object that needs to be classified.\n",
    "\n",
    "* **Question:** Now that we have formally defined our encoding of y, and we would have some images x, what is the final step needed before training the network?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The last step needed is to define the loss function.\n",
    "\n",
    "Given $y$ which is the true target and $\\hat{y}$. $y$ can be expressed as $y_1, y_2, \\dots, y_8$, where $y_1 = p_c$ and so on.\n",
    "\n",
    "So our loss function can be as follows:\n",
    "\n",
    "* if $y_1 = 0$ then $L(y,\\hat{y}) = (\\hat{y_1} - y_1)^2$ this makes sense since we don't care about the other values here as there is no object in the image.\n",
    "\n",
    "* if $y_1 = 1$ then $L(y,\\hat{y}) = (\\hat{y_1} - y_1)^2 + (\\hat{y_2} - y_2)^2 + \\dots + (\\hat{y_8} - y_8)^2$\n",
    "\n",
    "Note that ff you wanted to, you could have a cross entropy loss for the class values (softmax values).\n",
    "\n",
    "The above discussion was about object locationation, not detection.\n",
    "\n",
    "Next, we talk about detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2iwf8ksH5Yn"
   },
   "source": [
    "## Landmark detection\n",
    "\n",
    "What if we wanted to build an algorithm to detect facial key landmarks, such as the location of the eyes, nose and mouth. **Question: How would we modify the targets? What do we need to worry about in terms of the target labelling**\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*See image below.*\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1bM-NAzFdaUsPY_n5gcvqz_ZfzBcQ0W-E)\n",
    "\n",
    "* What applications (related to facial landmarks) can you think of for which this would be useful?\n",
    "\n",
    "* What applications (related to any field) can you think of for which this would be useful?\n",
    "\n",
    "* **Question: How could this idea to extended to pose estimation?**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "*See below.*\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1jnzmPP4PGdSjlCwYLqDbHf63l6gLZjze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvZn67dPjfE5"
   },
   "source": [
    "# Task: Implement a land mark detection algorithm\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "Each predicted keypoint is specified by an (x,y) real-valued pair in the space of pixel indices. There are 15 keypoints, which represent the following elements of the face:\n",
    "\n",
    "left_eye_center, right_eye_center, left_eye_inner_corner, left_eye_outer_corner, right_eye_inner_corner, right_eye_outer_corner, left_eyebrow_inner_end, left_eyebrow_outer_end, right_eyebrow_inner_end, right_eyebrow_outer_end, nose_tip, mouth_left_corner, mouth_right_corner, mouth_center_top_lip, mouth_center_bottom_lip\n",
    "\n",
    "Left and right here refers to the point of view of the subject.\n",
    "\n",
    "In some examples, some of the target keypoint positions are misssing (encoded as missing entries in the csv, i.e., with nothing between two commas).\n",
    "\n",
    "The input image is given in the last field of the data files, and consists of a list of pixels (ordered by row), as integers in (0,255). The images are 96x96 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcC0o7eGUqzd"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEOZmBcDsgV3"
   },
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Txog2sPZoBu"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "test_file = '1eVzKGRlHPCNqkwaZFz_ZM7Q0UNCiQ0h_'\n",
    "downloaded = drive.CreateFile({'id': test_file})\n",
    "downloaded.GetContentFile('test.csv')\n",
    "\n",
    "train_file = '1uLQ307pPe5bEOLHEzdb2CqOKfKZXTS2y'\n",
    "downloaded = drive.CreateFile({'id': train_file})\n",
    "downloaded.GetContentFile('training.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaSDBjtPsh73"
   },
   "source": [
    "## Unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5f9OeSadf5H"
   },
   "outputs": [],
   "source": [
    "!unzip \"training.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g09p2684sjpK"
   },
   "source": [
    "### Pre-process the data\n",
    "\n",
    "The data is a bit messy. It is represented in .csv files. Each row represents one instance of data. The row contains the keypoints and the image which is arranged in one vector. This function reshapes the image in a 96x96 image, and gets the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwhlCRZdbOZw"
   },
   "outputs": [],
   "source": [
    "def loadData(train, dFrame):\n",
    "  dFrame['Image'] = dFrame['Image'].apply(lambda row: np.fromstring(row, sep=' '))\n",
    "    \n",
    "  dFrame = dFrame.dropna()  # drop all rows that have missing values in them\n",
    "  X = np.vstack(dFrame['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
    "  X = X.astype(np.float32)\n",
    "  X = X.reshape(-1, 96, 96, 1) # return each images as 96 x 96 x 1\n",
    "  \n",
    "  if train:\n",
    "    y = dFrame[dFrame.columns[:-1]].values\n",
    "    y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "    X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "    y = y.astype(np.float32)\n",
    "  else:\n",
    "    y = None\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9kHdcw6s0MY"
   },
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw0aqj7ScVaR"
   },
   "outputs": [],
   "source": [
    "dFrame = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1Tm5pdbb-p0"
   },
   "outputs": [],
   "source": [
    "dFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LApCMtdehFET"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = loadData(True, dFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIYnzJKns2gH"
   },
   "source": [
    "## Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByHAyN_XhgN1"
   },
   "outputs": [],
   "source": [
    "dFrame = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8GXJZG3hjWU"
   },
   "outputs": [],
   "source": [
    "dFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFAQXCl5hZl5"
   },
   "outputs": [],
   "source": [
    "X_test, y_test = loadData(False, dFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re-_sy8Cs4M_"
   },
   "source": [
    "### Check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XHWAwiehQT4"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eUH_QDVhRZJ"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UBhPyYwhtc0"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoE-sjSks5jP"
   },
   "source": [
    "## Visualise some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO8XvIhlXjwS"
   },
   "outputs": [],
   "source": [
    "def plotData(image, landmarks, figure):\n",
    "    figure.imshow(np.squeeze(image), cmap='gray') # plot the image\n",
    "    landmarks = landmarks * 48 + 48 # undo the normalization\n",
    "    # Plot the keypoints\n",
    "    figure.scatter(landmarks[0::2],landmarks[1::2],marker='o',c='r',s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uiuo72LUuM5"
   },
   "outputs": [],
   "source": [
    "# visualize four taining samples\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "for i in range(4):\n",
    "     figure = fig.add_subplot(2, 2, i + 1, xticks=[], yticks=[])\n",
    "     plotData(X_train[i], y_train[i], figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jANp-FNioo_"
   },
   "source": [
    "# Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAEOx5HkiHu8"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Uq7OJfys_D5"
   },
   "source": [
    "## Predict on the test data\n",
    "\n",
    "How does the untrained model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNLRdATujy3D"
   },
   "outputs": [],
   "source": [
    "predictions = # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VU4NFgXxj1mh"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2, 2, i + 1, xticks=[], yticks=[])\n",
    "    plotData(X_test[i], predictions[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEd5DmPFtDOo"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Use model checkpoints to save the best model\n",
    "\n",
    "```\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath='model_weights.ckpt'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath, verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(... , callbacks = [checkpointer,hist] ... )\n",
    "\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Then to load back:\n",
    "\n",
    "model.load_weights(filepath)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bZTNgNQiXbB"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZ_eDst6tZnp"
   },
   "source": [
    "## Load the best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qT51Zv8iyw0"
   },
   "outputs": [],
   "source": [
    "# get the checkpoint file to load the weights\n",
    "# TO DO\n",
    "\n",
    "print(\"Test data shape == {}\".format(X_test.shape))\n",
    "\n",
    "predictions = # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ_u5LxLtb0a"
   },
   "source": [
    "## Visualise predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUlMLismjPtd"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2, 2, i + 1, xticks=[], yticks=[])\n",
    "    plotData(X_test[i], predictions[i], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jNs1-kdjS7R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
