{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da55336",
   "metadata": {
    "id": "3da55336"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization, SeparableConv2D\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782142b",
   "metadata": {
    "id": "0782142b",
    "origin_pos": 1
   },
   "source": [
    "# Xception\n",
    "\n",
    "## A discussion on separable convolution and variants\n",
    "\n",
    "* The spatial separable convolution is so named because it deals primarily with the spatial dimensions of an image and kernel: the width and the height\n",
    "\n",
    "* A spatial separable convolution simply divides a kernel into two, smaller kernels. The most common case would be to divide a 3x3 kernel into a 3x1 and 1x3 kernel, like so:\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*mL53fW0tJpNWEePp54y1Sg.png)\n",
    "\n",
    "* Now, instead of doing one convolution with 9 multiplications, we do two convolutions with 3 multiplications each (6 in total) to achieve the same effect. With less multiplications, computational complexity goes down, and the network is able to run faster.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*o3mKhG3nHS-1dWa_plCeFw.png)\n",
    "\n",
    "* The main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels.\n",
    "\n",
    "## Depthwise separable conv\n",
    "\n",
    "* Unlike spatial separable convolutions, **depthwise separable convolutions** work with kernels that cannot be “factored” into two smaller kernels. Hence, it is more commonly used. \n",
    "\n",
    "* The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension \n",
    "\n",
    "* An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. You can imagine each channel as a particular interpretation of that image; in for example, the “red” channel interprets the “redness” of each pixel, the “blue” channel interprets the “blueness” of each pixel, and the “green” channel interprets the “greenness” of each pixel. An image with 64 channels has 64 different interpretations of that image.\n",
    "\n",
    "* Let's consider normal 2d conv, and let us assume we only one to create one filter. Below, we end up doing $5\\times5\\times3$ multiplications when we calculate ***one*** value in the feature map.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*fgYepSWdgywsqorf3bdksg.png)\n",
    "\n",
    "* After going through a 5x5x3 kernel, the 12x12x3 image will become a 8x8x1 image.\n",
    "\n",
    "* The above example was for one filter, how about more? What if we want to increase the number of channels in our output image? What if we want an output of size 8x8x256? See below.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*XloAmCh5bwE4j1G7yk5THw.png)\n",
    "\n",
    "* Assuming we now want to create a feature map with a depth of 256, how many multiplications will we perform for each single feature map value now?\n",
    "\n",
    "* Introducing, depthwise separable convolution.\n",
    "\n",
    "* Made up of two main operations: depthwise convolution and a pointwise convolution.\n",
    "\n",
    "* Step 1: Depthwise convolution performs channel-wise $n\\times n$ spatial convolution. In the image below, this will be 3 5x5x1 convolutions. This will result in 25 multiplications. Each 5x5x1 kernel iterates 1 channel of the image (note: 1 channel, not all channels), getting the scalar products of every 25 pixel group, giving out a 8x8x1 image. Stacking these images together creates a 8x8x3 image\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*yG6z6ESzsRW-9q5F_neOsg.png)\n",
    "\n",
    "\n",
    "\n",
    "* Step 2: pointwise convolution (aka 1x1 conv). Remember, the original convolution transformed a 12x12x3 image to a 8x8x256 image. Currently, the depthwise convolution has transformed the 12x12x3 image to a 8x8x3 image. Now, we need to increase the number of channels of each image.\n",
    "\n",
    "* We iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*37sVdBZZ9VK50pcAklh8AQ.png)\n",
    "\n",
    "* And since we want 256 feature maps, we repeat this process 256 times. We can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*Q7a20gyuunpJzXGnWayUDQ.png) \n",
    "\n",
    "* Let’s calculate the number of multiplications the computer has to do in the original convolution. There are 256 5x5x3 kernels that move 8x8 times, each one is 3x5x5x8x8 = 4,800 multiplications. So far 256 filters, that’s 256x3x5x5x8x8=1,228,800 multiplications.\n",
    "\n",
    "* What about the separable convolution? \n",
    "\n",
    "* In the depthwise convolution, we have 3 5x5x1 kernels that move 8x8 times. That’s 3x5x5x8x8 = 4,800 multiplications. In the pointwise convolution, we have 256 1x1x3 kernels that move 8x8 times. That’s 256x1x1x3x8x8=49,152 multiplications. Adding them up together, that’s 53,952 multiplications.\n",
    "\n",
    "* 52,952 is a lot less than 1,228,800. With less computations, the network is able to process more in a shorter amount of time.\n",
    "\n",
    "## Xception (modified depthwise separable convolution)\n",
    "\n",
    "This is what we talked about, where we first apply depthwise and then pointwise convolution:\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*VvBTMkVRus6bWOqrK1SlLQ.png)\n",
    "\n",
    "* The modified depthwise separable convolution is the pointwise convolution followed by a depthwise convolution. This modification is motivated by the inception module in Inception-v3 that 1×1 convolution is done first before any n×n spatial convolutions. See below\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*J8dborzVBRBupJfvR7YhuA.png)\n",
    "\n",
    "\n",
    "\n",
    "* In the original Inception Module, there is non-linearity after the first operation. In Xception, the modified depthwise separable convolution, there is NO intermediate ReLU non-linearity.\n",
    "\n",
    "\n",
    "credits: \n",
    " \n",
    " https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728\n",
    "\n",
    " https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LQyko-xkSRQo",
   "metadata": {
    "id": "LQyko-xkSRQo"
   },
   "source": [
    "# Task: implement Xception\n",
    "\n",
    "To simplify the problem, use an input shape of (94,94,1) when you train the model, but when you build Xception, you should use the original shapes as per the paper, that is, (299,299,1).\n",
    "\n",
    "The network is essentially broken up into 3 parts: entry, middle and exit. Code each one individually using the Functional API.\n",
    "\n",
    "The shapes for each major block is provided to guide you.\n",
    "\n",
    "Some notes below to guide you\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1mwoViyi9FLhDgUBsxSsf6mmDC-Go7MXv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X5y_ruMWKhcH",
   "metadata": {
    "id": "X5y_ruMWKhcH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import SeparableConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gObXt3bCD2xE",
   "metadata": {
    "id": "gObXt3bCD2xE"
   },
   "outputs": [],
   "source": [
    "# Define some input, with 4 tensors of 8x8x3\n",
    "X = tf.random.uniform((4, 8, 8, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G9v0690pl22W",
   "metadata": {
    "id": "G9v0690pl22W"
   },
   "outputs": [],
   "source": [
    "sep = SeparableConv2D(filters = 2, kernel_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iujk6Ax2l24c",
   "metadata": {
    "id": "iujk6Ax2l24c"
   },
   "outputs": [],
   "source": [
    "sep(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zP_AYUFfEfwB",
   "metadata": {
    "id": "zP_AYUFfEfwB"
   },
   "source": [
    "## Entry flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "msooVI4--fPP",
   "metadata": {
    "id": "msooVI4--fPP"
   },
   "outputs": [],
   "source": [
    "# Use an input of 299,299,1\n",
    "\n",
    "# to simplify, call the small blocks as follows: stem, block2, block3, block4\n",
    "# Shapes for each are as follows\n",
    "\n",
    "#(None, 150, 150, 64)\n",
    "#(None, 75, 75, 128)\n",
    "#(None, 38, 38, 256)\n",
    "#(None, 19, 19, 728)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6IRj4VCgEoQY",
   "metadata": {
    "id": "6IRj4VCgEoQY"
   },
   "source": [
    "## Middle flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rsXsyVmnl26s",
   "metadata": {
    "id": "rsXsyVmnl26s"
   },
   "outputs": [],
   "source": [
    "# to do\n",
    "# for simplicity, only repeat 3 times\n",
    "# you can call them, middle1, middle2 and middle3\n",
    "\n",
    "# shapes as follows:\n",
    "\n",
    "#(None, 19, 19, 728)\n",
    "#(None, 19, 19, 728)\n",
    "#(None, 19, 19, 728)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T-SxOYQtHhuF",
   "metadata": {
    "id": "T-SxOYQtHhuF"
   },
   "source": [
    "## Exit flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anK4X2c-83la",
   "metadata": {
    "id": "anK4X2c-83la"
   },
   "outputs": [],
   "source": [
    "# to do\n",
    "\n",
    "# (None, 10, 10, 1024) shape after the addition operation\n",
    "# (None, 10, 10, 2048) after last relu\n",
    "# (None, 2048) # after global pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eqOURhgZJe0T",
   "metadata": {
    "id": "eqOURhgZJe0T"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X1GTR35cmP7v",
   "metadata": {
    "id": "X1GTR35cmP7v"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uQ_KjFJomP7x",
   "metadata": {
    "id": "uQ_KjFJomP7x"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z51sdOntmP78",
   "metadata": {
    "id": "z51sdOntmP78"
   },
   "source": [
    "## Find the unique numbers from the train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o29O_gUTmP79",
   "metadata": {
    "id": "o29O_gUTmP79"
   },
   "outputs": [],
   "source": [
    "classes = np.unique(Y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BA_mGK_NVHwn",
   "metadata": {
    "id": "BA_mGK_NVHwn"
   },
   "source": [
    "## Reshape needed\n",
    "\n",
    "Keras wants to know the depth of an image. \n",
    "\n",
    "For CNNS, Keras wants the format of the data as follows: [batches, width, height, depth]. \n",
    "\n",
    "In this case the colour channel/depth of the images is 1. Currently the shape is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ZpEbCB6kQ",
   "metadata": {
    "id": "a55ZpEbCB6kQ"
   },
   "source": [
    "But this doesn't have a depth value. So we can reshape it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D8HTp2YHVH4_",
   "metadata": {
    "id": "D8HTp2YHVH4_"
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UfClFp-6K7P6",
   "metadata": {
    "id": "UfClFp-6K7P6"
   },
   "source": [
    "## Convert from categorical labels to one-hot encoded vectors\n",
    "\n",
    "In this case there are 10 classes so we can tell the function to convert into a vector of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i3xjsPnVmP8j",
   "metadata": {
    "id": "i3xjsPnVmP8j"
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q7LVPnt1QPQM",
   "metadata": {
    "id": "Q7LVPnt1QPQM"
   },
   "source": [
    "## Small twist!\n",
    "\n",
    "API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PQNnZHhNSEKW",
   "metadata": {
    "id": "PQNnZHhNSEKW"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aHvmE76ySEMx",
   "metadata": {
    "id": "aHvmE76ySEMx"
   },
   "outputs": [],
   "source": [
    "def resize_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    image = tf.image.resize(image, (94,94))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fqEzyUPLSOdv",
   "metadata": {
    "id": "fqEzyUPLSOdv"
   },
   "outputs": [],
   "source": [
    "train_ds = (train_ds\n",
    "                  .map(resize_images)\n",
    "                  .shuffle(buffer_size=10000)\n",
    "                  .batch(batch_size=64, drop_remainder=True))\n",
    "test_ds = (test_ds\n",
    "                  .map(resize_images)\n",
    "                  .batch(batch_size=32, drop_remainder=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gozmsPxhmP8v",
   "metadata": {
    "id": "gozmsPxhmP8v"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6E64aSOxJzkf",
   "metadata": {
    "id": "6E64aSOxJzkf"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21MPAIDEK6c8",
   "metadata": {
    "id": "21MPAIDEK6c8"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, \"Xception.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ifPs0mftmP84",
   "metadata": {
    "id": "ifPs0mftmP84"
   },
   "source": [
    "## Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j8RZc1rhmP84",
   "metadata": {
    "id": "j8RZc1rhmP84"
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=2, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_j9LecHgmP8_",
   "metadata": {
    "id": "_j9LecHgmP8_"
   },
   "source": [
    "## Predict on all the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7TtfkDWjmP8_",
   "metadata": {
    "id": "7TtfkDWjmP8_"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b_zM7W80cFhJ",
   "metadata": {
    "id": "b_zM7W80cFhJ"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abcDBFmcCz5",
   "metadata": {
    "id": "0abcDBFmcCz5"
   },
   "outputs": [],
   "source": [
    "correct_values = np.argmax(Y_test,axis=-1)\n",
    "predicted_classes = np.argmax(predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N7keRohxmP9H",
   "metadata": {
    "id": "N7keRohxmP9H"
   },
   "outputs": [],
   "source": [
    "accuracy_score(predicted_classes,correct_values)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sT3V263AT1bQ",
   "metadata": {
    "id": "sT3V263AT1bQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
