{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxmDMK4yupqg"
   },
   "source": [
    "# Object Detection API - Your own images!\n",
    "\n",
    "\n",
    "## Download your own data first\n",
    "\n",
    "* Potentially find images from wikimedia: https://commons.wikimedia.org/wiki/Main_Page\n",
    "\n",
    "* Get a few images of the same size, say 512x512\n",
    "* Download them\n",
    "* Zip the files and upload them to Google drive\n",
    "* Next, download them/ or upload them directly here on the left!\n",
    "\n",
    "## You will fine-tune a ResNet object detector on your data\n",
    "\n",
    "Model Zoo where you can find the URL to the new model: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8XnRz0Pt43x"
   },
   "outputs": [],
   "source": [
    "# uncomment the next line if you want to delete an existing models directory\n",
    "!rm -rf ./models/\n",
    "\n",
    "# clone the Tensorflow Model Garden\n",
    "!git clone --depth 1 https://github.com/tensorflow/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwdsBdGhFanc"
   },
   "outputs": [],
   "source": [
    "# Compile the Object Detection API protocol buffers\n",
    "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEh51aSAz390"
   },
   "source": [
    "You will write a file based on the [setup script](https://github.com/tensorflow/models/blob/master/research/object_detection/packages/tf2/setup.py) in the official repo to work with the packages in the current version of Colab. We removed some the packages that is not needed in this lab to make the installation faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yWhz5fQzqSb"
   },
   "outputs": [],
   "source": [
    "%%writefile models/research/setup.py\n",
    "\n",
    "import os\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'tf-models-official==2.8.0',\n",
    "    'tensorflow_io'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='object_detection',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    include_package_data=True,\n",
    "    packages=(\n",
    "        [p for p in find_packages() if p.startswith('object_detection')] +\n",
    "        find_packages(where=os.path.join('.', 'slim'))),\n",
    "    package_dir={\n",
    "        'datasets': os.path.join('slim', 'datasets'),\n",
    "        'nets': os.path.join('slim', 'nets'),\n",
    "        'preprocessing': os.path.join('slim', 'preprocessing'),\n",
    "        'deployment': os.path.join('slim', 'deployment'),\n",
    "        'scripts': os.path.join('slim', 'scripts'),\n",
    "    },\n",
    "    description='Tensorflow Object Detection Library',\n",
    "    python_requires='>3.6',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq1-SiyB0YTP"
   },
   "outputs": [],
   "source": [
    "# Run the setup script you just wrote\n",
    "!python -m pip install models/research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21tUtyyVrkIt"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Let's now import the packages you will use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZcqD4NLdnf4"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Import Object Detection API packages\n",
    "\n",
    "# import the label map utility module\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "# import module for reading and updating configuration files.\n",
    "from object_detection.utils import config_util\n",
    "\n",
    "# import module for visualization. use the alias `viz_utils`\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "# import module for building the detection model\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "# import module for utilities in Colab\n",
    "from object_detection.utils import colab_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IogyryF2lFBL"
   },
   "source": [
    "## Utilities\n",
    "\n",
    "You'll define a couple of utility functions for loading images and plotting detections. This code is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y9R0Xllefec"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "    path: a file path.\n",
    "\n",
    "    Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(img_data))\n",
    "    (im_width, im_height) = image.size\n",
    "    \n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "    \"\"\"Wrapper function to visualize detections.\n",
    "\n",
    "    Args:\n",
    "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    boxes: a numpy array of shape [N, 4]\n",
    "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
    "          and match the keys in the label map.\n",
    "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
    "          this function assumes that the boxes to be plotted are groundtruth\n",
    "          boxes and plot all boxes as black with no classes or scores.\n",
    "    category_index: a dict containing category dictionaries (each holding\n",
    "          category index `id` and category name `name`) keyed by category indices.\n",
    "    figsize: size for the figure.\n",
    "    image_name: a name for the image file.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_np_with_annotations = image_np.copy()\n",
    "    \n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_annotations,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        min_score_thresh=0.8)\n",
    "    \n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "    \n",
    "    else:\n",
    "        plt.imshow(image_np_with_annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSaXL28TZfk1"
   },
   "source": [
    "## Download your own data\n",
    "\n",
    "* Potentially find images from wikimedia: https://commons.wikimedia.org/wiki/Main_Page\n",
    "\n",
    "* Get a few images of the same size, say 512x512\n",
    "* Download them\n",
    "* Zip the files and upload them to Google drive\n",
    "* Next, download them/ or upload them directly here on the left!\n",
    "\n",
    "If you upload to Google Drive, you need to make sure the zip file is shareable (publicly) then get the file ID by coping the link and extracting the ID part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUccO5_AI1zS"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "train_file = '1-8GtxXpzoKUPd9uHE68KvuGxRgFPdM3H'\n",
    "downloaded = drive.CreateFile({'id': train_file})\n",
    "downloaded.GetContentFile('Pieds.zip')\n",
    "\n",
    "# unzip to a local directory\n",
    "local_zip = './Pieds.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./training')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyzSGUDsrkI7"
   },
   "source": [
    "### Visualize the training images\n",
    "\n",
    "Next, you'll want to inspect the images that you just downloaded. \n",
    "\n",
    "* You can inspect the *training* directory (using the `Files` button on the left side of this Colab) to see the filenames of the zombie images. The paths for the images will look like this:\n",
    "\n",
    "```\n",
    "./training/pied-kingfisher-1.jpg\n",
    "./training/pied-kingfisher-2.jpg\n",
    "./training/pied-kingfisher-3.jpg\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQy3ND7EpFQM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# assign the name (string) of the directory containing the training images\n",
    "train_image_dir = './training'\n",
    "\n",
    "# declare an empty list\n",
    "train_images_np = []\n",
    "\n",
    "# run a for loop for each image\n",
    "for i in range(1, 8):\n",
    "\n",
    "    # define the path (string) for each image\n",
    "    image_path = os.path.join(train_image_dir, \"pied-kingfisher-\" +str(i) + \".jpeg\")\n",
    "    print(image_path)\n",
    "\n",
    "    # load images into numpy arrays and append to a list\n",
    "    train_images_np.append(load_image_into_numpy_array(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pngfC4_iNdkp"
   },
   "source": [
    "## Check the shapes of your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r36GFMboNUXS"
   },
   "outputs": [],
   "source": [
    "for i in range (len(train_images_np)):\n",
    "  print(train_images_np[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dqb_yjAo3cO_"
   },
   "source": [
    "<a name='gt_boxes_definition'></a>\n",
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdKtOxZGXoy1"
   },
   "source": [
    "In this section, you will create your ground truth boxes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqDVC6epheJZ"
   },
   "outputs": [],
   "source": [
    "# Define the list of ground truth boxes\n",
    "gt_boxes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nEDRoUEcUgL"
   },
   "outputs": [],
   "source": [
    "# annotate the training images\n",
    "colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olhzhCH5heJa"
   },
   "source": [
    "#### View your ground truth box coordinates\n",
    "Whether you chose to draw your own or use the given boxes, please check your list of ground truth box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxd66yq-M9Va"
   },
   "outputs": [],
   "source": [
    "# print the coordinates of your ground truth boxes\n",
    "for gt_box in gt_boxes:\n",
    "  print(gt_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeDQaSdrkJC"
   },
   "source": [
    "### Define the category index dictionary\n",
    "\n",
    "You'll need to tell the model which integer class ID to assign to the 'zombie' category, and what 'name' to associate with that integer id.\n",
    "\n",
    "- zombie_class_id: By convention, class ID integers start numbering from 1,2,3, onward.\n",
    "  - If there is ever a 'background' class, it could be assigned the integer 0, but in this case, you're just predicting the one zombie class.\n",
    "  - Since you are just predicting one class (zombie), we assign `1` to the zombie class ID.\n",
    "\n",
    "- category_index: we need to define the `category_index` dictionary, which will have the same structure as this:\n",
    "```\n",
    "{human_class_id : \n",
    "  {'id'  : human_class_id, \n",
    "   'name': 'human_so_far'}\n",
    "}\n",
    "```\n",
    "- num_classes: Since you are predicting one class, assign `1` to the number of classes that the model will predict.\n",
    "  - This will be used during data preprocessing and again when you configure the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWBqFVMcweF-"
   },
   "outputs": [],
   "source": [
    "# Assign the zombie class ID\n",
    "kf_class_id = 1\n",
    "\n",
    "# define a dictionary describing the zombie class\n",
    "category_index = {kf_class_id : \n",
    "{'id'  : 1, \n",
    " 'name': 'kingfisher'}\n",
    "}\n",
    "\n",
    "# Specify the number of classes that the model will predict\n",
    "num_classes = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTvj7Q0oYk75"
   },
   "source": [
    "### Data preprocessing\n",
    "You will now do some data preprocessing so it is formatted properly before it is fed to the model:\n",
    "- Convert the class labels to one-hot representations\n",
    "- convert everything (i.e. train images, gt boxes and class labels) to tensors.\n",
    "\n",
    "This code is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H68ot6BkrkJH"
   },
   "outputs": [],
   "source": [
    "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
    "# we do this here so that the model receives one-hot labels where non-background\n",
    "# classes start counting at the zeroth index. \n",
    "\n",
    "label_id_offset = 1\n",
    "train_image_tensors = []\n",
    "\n",
    "# lists containing the one-hot encoded classes and ground truth boxes\n",
    "gt_classes_one_hot_tensors = []\n",
    "gt_box_tensors = []\n",
    "\n",
    "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
    "    \n",
    "    # convert training image to tensor, add batch dimension, and add to list\n",
    "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
    "        train_image_np, dtype=tf.float32), axis=0))\n",
    "    \n",
    "    # convert numpy array to tensor, then add to list\n",
    "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
    "    \n",
    "    # apply offset to to have zero-indexed ground truth classes\n",
    "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
    "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
    "    \n",
    "    # do one-hot encoding to ground truth classes\n",
    "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
    "        zero_indexed_groundtruth_classes, num_classes))\n",
    "\n",
    "print('Done prepping data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTRrVo3N2IvJ"
   },
   "outputs": [],
   "source": [
    "gt_classes_one_hot_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvwNVmrt3R3A"
   },
   "source": [
    "## Visualize the images with their ground truth bounding boxes\n",
    "\n",
    "You should see the 5 training images with the bounding boxes after running the cell below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SmCUwvP3R3A"
   },
   "outputs": [],
   "source": [
    "# give boxes a score of 100%\n",
    "dummy_scores = np.array([1.0], dtype=np.float32)\n",
    "\n",
    "# define the figure size\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# use the `plot_detections()` utility function to draw the ground truth boxes\n",
    "for idx in range(5):\n",
    "    plt.subplot(2, 4, idx+1)\n",
    "    plot_detections(\n",
    "      train_images_np[idx],\n",
    "      gt_boxes[idx],\n",
    "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
    "      dummy_scores, category_index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn21-EelheJa"
   },
   "source": [
    "### Download checkpoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J16r3NChD-7"
   },
   "outputs": [],
   "source": [
    "# Download the SSD Resnet 50 version 1, 640x640 checkpoint\n",
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    \n",
    "# untar (decompress) the tar file\n",
    "!tar -xf ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "\n",
    "# copy the checkpoint to the test_data folder models/research/object_detection/test_data/\n",
    "!mv ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KReUvbyL5I7n"
   },
   "source": [
    "- In the Colab, on the left side table of contents, click on the folder icon to display the file browser for the current workspace.  \n",
    "- Navigate to `models/research/object_detection/configs/tf2`.  The folder has multiple .config files.  \n",
    "- Look for the file corresponding to faster rcnn resnet50 v1 640x640\n",
    "- You can double-click the config file to view its contents. \n",
    "- Set the `pipeline_config` to a string that contains the full path to the resnet config file, in other words: `models/research/.../... .config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEcu2rN62I3O"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# define the path to the .config file for ssd resnet 50 v1 640x640\n",
    "pipeline_config = '/content/models/research/object_detection/configs/tf2/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "\n",
    "# Load the configuration file into a dictionary\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "\n",
    "# See what configs looks like\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxHqQK1eheJa"
   },
   "source": [
    "## Get the model config\n",
    "\n",
    "This tells us details about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYwjLv2M6C8I"
   },
   "outputs": [],
   "source": [
    "# Read in the object stored at the key 'model' of the configs dictionary\n",
    "model_config = configs.get('model')\n",
    "\n",
    "# see what model_config looks like\n",
    "model_config\n",
    "\n",
    "# Notice something on the 2nd line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuDDfCxd6TZf"
   },
   "source": [
    "\n",
    "### Modify model_config\n",
    "- Modify num_classes from the default `90` to the `num_classes` that you set earlier in this notebook.\n",
    "- Freeze batch normalization \n",
    "  - Batch normalization is not frozen in the default configuration.\n",
    "  - If you inspect the `model_config` object, you'll see that `freeze_batchnorm` is nested under `ssd` just like `num_classes`.\n",
    "  - Freeze batch normalization by setting the relevant field to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyT4BUbaMeG-"
   },
   "outputs": [],
   "source": [
    "# Modify the number of classes from its default of 90 to 1\n",
    "model_config.ssd.num_classes = 1\n",
    "\n",
    "# Freeze batch normalization from False to True\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "\n",
    "# See what model_config now looks like after you've customized it!\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twje2tVU7kyn"
   },
   "source": [
    "## Model builder\n",
    "\n",
    "We included this library earlier.\n",
    "\n",
    "Now we build the model and pass it the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZTJ0oOUvc8N"
   },
   "outputs": [],
   "source": [
    "detection_model = model_builder.build(model_config=model_config, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1sJVL7O9paW"
   },
   "source": [
    "View what you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8XKl2MU85mi"
   },
   "outputs": [],
   "source": [
    "vars(detection_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALXMga819dOX"
   },
   "source": [
    "### Isolate layers we want to re-use\n",
    "You will now isolate the layers of detection_model that you wish to reuse so that you can restore the weights to just those layers.\n",
    "\n",
    "You'll see that detection_model contains several variables. Two of these will be relevant to you:\n",
    "```\n",
    "...\n",
    "_box_predictor': <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f5205eeb1d0>,\n",
    "...\n",
    "_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f52040f1ef0>,\n",
    "```\n",
    "\n",
    "Your end goal is to create a custom model which reuses parts of, but not all of the layers of RetinaNet (currently stored in ``detection_model.``)\n",
    "\n",
    "The parts of RetinaNet that you want to reuse are: (here I specifically denoted them as 1 and 2 so the explanations are a bit easier to follow below)\n",
    "\n",
    "1. Bounding box regression prediction layer (`_box_predictor`)\n",
    "2. Feature extraction layers (`_feature_extractor`)\n",
    "\n",
    "The part of RetinaNet that you **will not want to reuse** is the classification prediction layer (since you will define and train your own classification layer specific to zombies). The current classification prediction layer was trained on another dataset.\n",
    "\n",
    "For the parts of RetinaNet that you want to reuse, you will also restore the weights from the checkpoint that you selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1Nz2sTPmXXx"
   },
   "source": [
    "`detection_model.box_predictor` contains a few things we will use:\n",
    "1. _prediction_head \n",
    "2. _base_tower_layers_for_heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2qnqEI3XQkB"
   },
   "outputs": [],
   "source": [
    "vars(detection_model._box_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ks616JZnNBf"
   },
   "source": [
    "Among the variables listed, a few will be relevant to you:\n",
    "\n",
    "```\n",
    "...\n",
    "_base_tower_layers_for_heads\n",
    "...\n",
    "_box_prediction_head\n",
    "...\n",
    "```\n",
    "\n",
    "First of all, where is box_predictor:\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py \n",
    "\n",
    "1. **Let's consider ``_base_tower_layers_for_heads``: in the code we find this:**\n",
    "https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/object_detection/predictors/convolutional_keras_box_predictor.py#L385\n",
    "```\n",
    "    # Stack the base_tower_layers in the order of conv_layer, batch_norm_layer\n",
    "    # and activation_layer\n",
    "    base_tower_layers = []\n",
    "    for i in range(self._num_layers_before_predictor):\n",
    "      base_tower_layers.extend([conv_layers[i]])\n",
    "```\n",
    "\n",
    "So `detection_model.box_predictor._base_tower_layers_for_heads` contains:\n",
    "\n",
    "* The layers for the prediction before the final bounding box prediction\n",
    "* The layers for the prediction before the final class prediction.\n",
    "\n",
    "\n",
    "2. **Let's consider ``_box_prediction_head``: in the code we find this:**\n",
    "https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/object_detection/predictors/convolutional_keras_box_predictor.py#L248\n",
    "\n",
    "```\n",
    "box_prediction_head: The head that predicts the boxes.\n",
    "```\n",
    "\n",
    "This points to the bounding box prediction layer, which you'll want to use for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKrcbAZJXq0R"
   },
   "source": [
    "### Now create the checkpoint based on what we want to reuse\n",
    "\n",
    "``_box_predictor`` has two objected nested in it, namely ``_base_tower_layers_for_heads`` and ``_box_prediction_head``. Since these are nested, we need to create the checkpoints for these separately. We can't create one giant checkpoint. This is why the code below is split into two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3fiaRRPv_Zp"
   },
   "outputs": [],
   "source": [
    "detection_model._ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqybrNYd8afs"
   },
   "outputs": [],
   "source": [
    "tmp_box_predictor_checkpoint = tf.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads = detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    _box_prediction_head = detection_model._box_predictor._box_prediction_head\n",
    ")  \n",
    "\n",
    "tmp_model_checkpoint = tf.train.Checkpoint(\n",
    "    _box_predictor = tmp_box_predictor_checkpoint, # We want the bounding box regression layer\n",
    "    _feature_extractor = detection_model._feature_extractor # We want the feature extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a_wdIJKEhAH"
   },
   "source": [
    "Now let's restore the checkpoint from the local disk based on what we downloaded and what we have defined that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsLloJ3DCAT_"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "\n",
    "# Define a checkpoint\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    model=tmp_model_checkpoint\n",
    ")\n",
    "\n",
    "# Restore the checkpoint to the checkpoint path\n",
    "checkpoint.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgYp1IgqFXss"
   },
   "source": [
    "## Actually load the weights\n",
    "\n",
    "As it stands, we defined how the weights should be loaded but haven't actually loaded them yet. This is probably since the model isn't in eager mode. With Tensorflow, everything is in the graph. So here, we pass through some data to run through the computation graph and load the weights. See below, there are no weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aQDc-DME61C"
   },
   "outputs": [],
   "source": [
    "# use the detection model's `preprocess()` method and pass a dummy image\n",
    "tmp_image, tmp_shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Icz7MTT7PAZb"
   },
   "outputs": [],
   "source": [
    "# run a prediction with the preprocessed image and shapes\n",
    "tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpc1eIWTPCfB"
   },
   "outputs": [],
   "source": [
    "len(detection_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkm9-AOoF94r"
   },
   "source": [
    "Weights restored!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rwQI8c_Pje2"
   },
   "source": [
    "### Next, some hyper-parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVtkhqk0FBs7"
   },
   "outputs": [],
   "source": [
    "# set the batch_size\n",
    "batch_size = 7\n",
    "\n",
    "# set the number of batches\n",
    "num_batches = 140\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# set the optimizer and pass in the learning_rate\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Em0xoxvPmHY"
   },
   "source": [
    "## Decide on which layers to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4b4HdoHF-qz"
   },
   "outputs": [],
   "source": [
    "# Inspect the layers of detection_model\n",
    "for i,v in enumerate(detection_model.trainable_variables):\n",
    "    print(f\"i: {i} \\t name: {v.name} \\t shape:{v.shape} \\t dtype={v.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07gGwaGGRYS4"
   },
   "source": [
    "Notice that there are some layers whose names are prefixed with the following:\n",
    "\n",
    "```\n",
    "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead\n",
    "...\n",
    "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead\n",
    "...\n",
    "WeightSharedConvolutionalBoxPredictor/BoxPredictionTower\n",
    "...\n",
    "WeightSharedConvolutionalBoxPredictor/ClassPredictionTower\n",
    "...\n",
    "```\n",
    "We want to fine-tune these ones:\n",
    "\n",
    "* The **bounding box head** variables (which predict bounding box coordinates - so we will want to fine-tune this)\n",
    "* The **class head** variables (which predict the class/category - so we will want to fine-tune this)\n",
    "\n",
    "And the rest?\n",
    "\n",
    "* \"tower\" refers to layers that are before the prediction layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtGSnewHGNJK"
   },
   "outputs": [],
   "source": [
    "# define a list that contains the layers that you wish to fine tune\n",
    "to_fine_tune = []\n",
    "for v in detection_model.trainable_variables:\n",
    "  if v.name.startswith('WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutional'):\n",
    "    to_fine_tune.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELjKtL89p05d"
   },
   "source": [
    "### Function to perform one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9o679B_HRJC"
   },
   "outputs": [],
   "source": [
    "# decorate with @tf.function for faster training\n",
    "@tf.function\n",
    "def train_step_fn(image_list,\n",
    "                groundtruth_boxes_list,\n",
    "                groundtruth_classes_list,\n",
    "                model,\n",
    "                optimizer,\n",
    "                vars_to_fine_tune):\n",
    "    \"\"\"A single training iteration.\n",
    "\n",
    "    Args:\n",
    "      image_list: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
    "        Note that the height and width can vary across images, as they are\n",
    "        reshaped within this function to be 640x640.\n",
    "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
    "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
    "        with type tf.float32 representing groundtruth boxes for each image in\n",
    "        the batch.\n",
    "\n",
    "    Returns:\n",
    "      A scalar tensor representing the total loss for the input batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Provide the ground truth to the model\n",
    "    model.provide_groundtruth(\n",
    "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "        groundtruth_classes_list=groundtruth_classes_list\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Preprocess the images\n",
    "        preprocessed_image_list = []\n",
    "        true_shape_list = []\n",
    "\n",
    "        for img in image_list:\n",
    "            processed_img, true_shape = model.preprocess(img)\n",
    "            preprocessed_image_list.append(processed_img)\n",
    "            true_shape_list.append(true_shape)\n",
    "\n",
    "        preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
    "        true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
    "\n",
    "        # Calculate the total loss (sum of both losses)\n",
    "        losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
    "        \n",
    "        total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "\n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient([total_loss], vars_to_fine_tune)\n",
    "\n",
    "        # Optimize the model's selected variables\n",
    "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    " \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uBLtu8qSYlL"
   },
   "source": [
    "## Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9t00XxYH8P2"
   },
   "outputs": [],
   "source": [
    "print('Start fine-tuning!', flush=True)\n",
    "\n",
    "for idx in range(num_batches):\n",
    "    # Grab keys for a random subset of examples\n",
    "    all_keys = list(range(len(train_images_np)))\n",
    "    random.shuffle(all_keys)\n",
    "    example_keys = all_keys[:batch_size]\n",
    "\n",
    "    # Get the ground truth\n",
    "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "    \n",
    "    # get the images\n",
    "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "\n",
    "    # Training step (forward pass + backwards pass)\n",
    "    total_loss = train_step_fn(image_tensors, \n",
    "                               gt_boxes_list, \n",
    "                               gt_classes_list,\n",
    "                               detection_model,\n",
    "                               optimizer,\n",
    "                               to_fine_tune\n",
    "                              )\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
    "        + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
    "\n",
    "print('Done fine-tuning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1-4BGyJMKnb"
   },
   "source": [
    "## Download some test images\n",
    "\n",
    "Follow same process as for your training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUeVBx11H_bM"
   },
   "outputs": [],
   "source": [
    "# uncomment if you want to delete existing files\n",
    "!rm -rf ./results\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "train_file = '1s82FBELO3xCCQVYj1OB3mgDq58ZJh2WR'\n",
    "downloaded = drive.CreateFile({'id': train_file})\n",
    "downloaded.GetContentFile('Pieds-Test.zip')\n",
    "\n",
    "# unzip test images\n",
    "local_zip = './Pieds-Test.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./results')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIRj-kjlMOMw"
   },
   "source": [
    "## Load them into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUroMHrKJFFo"
   },
   "outputs": [],
   "source": [
    "test_image_dir = './results/'\n",
    "test_images_np = []\n",
    "\n",
    "# load images into a numpy array. this will take a few minutes to complete.\n",
    "# You only need to do this once\n",
    "for i in range(1, 5):\n",
    "    image_path = os.path.join(test_image_dir, 'test-' + str(i) + '.jpeg')\n",
    "    print(image_path)\n",
    "    test_images_np.append(np.expand_dims(\n",
    "      load_image_into_numpy_array(image_path), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqUAAnI2MZII"
   },
   "source": [
    "We have 4 testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZNov959MXPj"
   },
   "outputs": [],
   "source": [
    "len(test_images_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5ZsthTmMamY"
   },
   "source": [
    "Each has the following dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6LSFI8sMRGk"
   },
   "outputs": [],
   "source": [
    "test_images_np[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmHTBKLUKYt0"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect(input_tensor):\n",
    "    \"\"\"Run detection on an input image.\n",
    "\n",
    "    Args:\n",
    "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
    "      Note that height and width can be anything since the image will be\n",
    "      immediately resized according to the needs of the model within this\n",
    "      function.\n",
    "\n",
    "    Returns:\n",
    "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
    "      and `detection_scores`).\n",
    "    \"\"\"\n",
    "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "    \n",
    "    # use the detection model's postprocess() method to get the the final detections\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESUuIQQCL9Sl"
   },
   "source": [
    "## Visualise results\n",
    "\n",
    "Results will appear in the /results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmKxKd_1Khfd"
   },
   "outputs": [],
   "source": [
    "label_id_offset = 1\n",
    "results = {'boxes': [], 'scores': []}\n",
    "\n",
    "for i in range(len(test_images_np)):\n",
    "    input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
    "    detections = detect(input_tensor)\n",
    "    plot_detections(\n",
    "      test_images_np[i][0],\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
    "      + label_id_offset,\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index, figsize=(15, 20), image_name=\"./results/gif_frame_\" + ('%03d' % i) + \".jpg\")\n",
    "    results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
    "    results['scores'].append(detections['detection_scores'][0][0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogb7fN81L_Iy"
   },
   "source": [
    "## Visualise a single result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xz2Efk9tLvJL"
   },
   "outputs": [],
   "source": [
    "display(IPyImage('./results/gif_frame_002.jpg'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXf0Uw8TL2C7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
