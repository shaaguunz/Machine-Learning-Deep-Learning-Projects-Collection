{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs3Vxo5dmP7j"
   },
   "source": [
    "## Image classification GoogLeNet\n",
    "\n",
    "***NOTE***\n",
    "\n",
    "Be sure to use hardware acceleration to use the GPU. Click on `Runtime`, change `runtime type`, and select `GPU` for the *hardware accelerator* option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FUHcE89LT3x"
   },
   "source": [
    "* Add batch nomalisation to GoogLeNet\n",
    "* Remove the preprocessing which was applied in the ```resize_images``` function which was responsible to for the normalisation, and instead, add a batch normalisation layer before the first convolutional layer in the network. You will run into problems calling the ```summary()``` function depending on where you add batch norm.\n",
    "* How does the performance compare in both cases versus GoogLeNet without batch normalisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpvo-bqTmP7m"
   },
   "source": [
    "## Imports first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvCXvXq5mP7p",
    "outputId": "5807d791-13ed-4336-86bf-3f3d64bc468f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56fx-uBSR070"
   },
   "source": [
    "# Module 1\n",
    "The first module uses a 64-channel 7x7 convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4qzSC_TB4U5"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization())\n",
    "# 2D conv\n",
    "model.add(tf.keras.layers.Conv2D(64, 7, strides=2, padding='same',\n",
    "                                   activation='relu', input_shape = (96,96,1)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "# Max pooling\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2,\n",
    "                                      padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxFQlI9aSrMV"
   },
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLpaJcBQSqxx"
   },
   "outputs": [],
   "source": [
    "# 1x1 conv\n",
    "model.add(tf.keras.layers.Conv2D(64, 1, activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2D 3x3 conv\n",
    "model.add(tf.keras.layers.Conv2D(192, 3, padding='same', activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Pooling\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgbJzOtrS2GP"
   },
   "outputs": [],
   "source": [
    "class Inception(tf.keras.Model):\n",
    "    # `c1`--`c4` are the number of output channels for each branch\n",
    "    def __init__(self, c1, c2, c3, c4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Branch 1 - doesn't change width or height\n",
    "        self.b1_1 = tf.keras.layers.Conv2D(c1, 1, activation='relu')\n",
    "        self.b1_1_bn = BatchNormalization()\n",
    "\n",
    "        # Branch 2\n",
    "        # 1x1 doesn't change width or height, but does change depth to 96\n",
    "        self.b2_1 = tf.keras.layers.Conv2D(c2[0], 1, activation='relu')\n",
    "        self.b2_1_bn = BatchNormalization()\n",
    "        # 2x2 can change width and height, but same padding is used! depth is 128 filters so depth of 128\n",
    "        self.b2_2 = tf.keras.layers.Conv2D(c2[1], 3, padding='same',\n",
    "                                           activation='relu')\n",
    "        self.b2_2_bn = BatchNormalization()\n",
    "        \n",
    "        # Branch 3\n",
    "        # 1x1 doesn't change width or height, but does change depth to 16\n",
    "        self.b3_1 = tf.keras.layers.Conv2D(c3[0], 1, activation='relu')\n",
    "        self.b3_1_bn = BatchNormalization()\n",
    "        # 2x2 can change width and height, but same padding is used! depth is 32 filters so depth of 32\n",
    "        self.b3_2 = tf.keras.layers.Conv2D(c3[1], 5, padding='same',\n",
    "                                           activation='relu')\n",
    "        self.b3_2_bn = BatchNormalization()\n",
    "        \n",
    "        # Branch 4\n",
    "        # max pooling doesn't change depth, but can change width and height. Padding is used to prevent this\n",
    "        self.b4_1 = tf.keras.layers.MaxPool2D(3, 1, padding='same')\n",
    "        self.b4_1_bn = BatchNormalization()\n",
    "        # 1x1 doesn't change width or height, but does change depth to 32\n",
    "        self.b4_2 = tf.keras.layers.Conv2D(c4, 1, activation='relu')\n",
    "        self.b4_2_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        # Branch 1 is applied to the input data\n",
    "        b1 = self.b1_1_bn(self.b1_1(x))\n",
    "\n",
    "        # Branch 2 is first applied to input and then to the output of the resulting feature map\n",
    "        # self.b2_1(x) is done first, and then self.b2_2()\n",
    "        b2 = self.b2_2_bn(self.b2_2(self.b2_1_bn(self.b2_1(x))))\n",
    "\n",
    "        # Branch 3 is first applied to input and then to the output of the resulting feature map\n",
    "        # self.b3_1(x) is done first, and then self.b3_2()\n",
    "        b3 = self.b3_2_bn(self.b3_2(self.b3_1_bn(self.b3_1(x))))\n",
    "\n",
    "        # Branch 4 is applied to input\n",
    "        b4 = self.b4_2_bn(self.b4_2(self.b4_1_bn(self.b4_1(x))))\n",
    "\n",
    "        # Concatenate\n",
    "        return tf.keras.layers.Concatenate()([b1, b2, b3, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gfqiuHHS2It"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(64, (96, 128), (16, 32), 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHNvd9UXWJMh"
   },
   "source": [
    "The number of output channels of the second Inception block is increased to 128+192+96+64=480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mTO2-pbSq29"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(128, (128, 192), (32, 96), 64))\n",
    "\n",
    "# Remember based on the diagram above, a pooling layer is applied after each module\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKle16IdWXoV"
   },
   "source": [
    "# Module 4\n",
    "The fourth module is more complicated. It connects five Inception blocks in series\n",
    "\n",
    "* block 1 outputs: 192+208+48+64 = 512\n",
    "\n",
    "* block 2 outputs: 160+224+64+64 = 512\n",
    "\n",
    "* block 3 outputs: 128+526+64+64 = 512\n",
    "\n",
    "* block 3 outputs: 112+228+64+64 = 528\n",
    "\n",
    "* block 5 outputs: 256+320+128+128 = 832\n",
    "\n",
    "Each block is learning a different number of features due to usual reasons but also due to difference in number of channels from 1x1 conv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTtB4f9jRKo7"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(192, (96, 208), (16, 48), 64))\n",
    "model.add(Inception(160, (112, 224), (24, 64), 64))\n",
    "model.add(Inception(128, (128, 256), (24, 64), 64))\n",
    "model.add(Inception(112, (144, 288), (32, 64), 64))\n",
    "model.add(Inception(256, (160, 320), (32, 128), 128))\n",
    "\n",
    "# Remember based on the diagram above, a pooling layer is applied after each module\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsex3G6fX7ps"
   },
   "source": [
    "# Module 5\n",
    "\n",
    "Two inception blocks\n",
    "\n",
    "* block 1 output: 256+320+128+128=832\n",
    "\n",
    "* block 2 output: 384+384+128+128=1024\n",
    "\n",
    "* This block uses the global average pooling layer to change the height and width of each channel to 1, just as in NiN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h3DNre8W5Gy"
   },
   "outputs": [],
   "source": [
    "model.add(Inception(256, (160, 320), (32, 128), 128))\n",
    "model.add(Inception(384, (192, 384), (48, 128), 128))\n",
    "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0hnnNudY23M"
   },
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDGsJOyqW5PQ"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvLNnz0vDnHc"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1GTR35cmP7v"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQ_KjFJomP7x",
    "outputId": "04e813c8-4bb7-4565-f4a7-fc35db8d445c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z51sdOntmP78"
   },
   "source": [
    "## Find the unique numbers from the train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o29O_gUTmP79",
    "outputId": "3babbb0f-cd44-49b7-a45b-a984836013af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  10\n",
      "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(Y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA_mGK_NVHwn"
   },
   "source": [
    "## Reshape needed\n",
    "\n",
    "Keras wants to know the depth of an image. \n",
    "\n",
    "For CNNS, Keras wants the format of the data as follows: [batches, width, height, depth]. \n",
    "\n",
    "In this case the colour channel/depth of the images is 1. Currently the shape is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a55ZpEbCB6kQ"
   },
   "source": [
    "But this doesn't have a depth value. So we can reshape it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8HTp2YHVH4_"
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfClFp-6K7P6"
   },
   "source": [
    "## Convert from categorical labels to one-hot encoded vectors\n",
    "\n",
    "In this case there are 10 classes so we can tell the function to convert into a vector of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3xjsPnVmP8j"
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7LVPnt1QPQM"
   },
   "source": [
    "## Small twist!\n",
    "\n",
    "API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQNnZHhNSEKW"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHvmE76ySEMx"
   },
   "outputs": [],
   "source": [
    "def resize_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    #image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    image = tf.image.resize(image, (96,96))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqEzyUPLSOdv"
   },
   "outputs": [],
   "source": [
    "train_ds = (train_ds\n",
    "                  .map(resize_images)\n",
    "                  .shuffle(buffer_size=10000)\n",
    "                  .batch(batch_size=64, drop_remainder=True))\n",
    "test_ds = (test_ds\n",
    "                  .map(resize_images)\n",
    "                  .batch(batch_size=32, drop_remainder=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gozmsPxhmP8v"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifPs0mftmP84"
   },
   "source": [
    "## Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8RZc1rhmP84",
    "outputId": "e9c5c826-8166-4211-8c41-6c4d74ddc7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "937/937 [==============================] - 88s 73ms/step - loss: 0.5405 - accuracy: 0.8195\n",
      "Epoch 2/2\n",
      "937/937 [==============================] - 69s 73ms/step - loss: 0.3647 - accuracy: 0.8678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3320190410>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=2, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j9LecHgmP8_"
   },
   "source": [
    "## Predict on all the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TtfkDWjmP8_",
    "outputId": "8b01ddff-6a78-4468-cc80-5b841b349f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_zM7W80cFhJ",
    "outputId": "b5ea1ba1-9fc9-4ba6-a255-a216b53dc146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0abcDBFmcCz5"
   },
   "outputs": [],
   "source": [
    "correct_values = np.argmax(Y_test,axis=-1)\n",
    "predicted_classes = np.argmax(predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7keRohxmP9H",
    "outputId": "0e207037-d094-4a4a-c1f5-bef269a8533f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted_classes,correct_values)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1Ik7Odwj9L6"
   },
   "source": [
    "More efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agkf9IKkiotI"
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "for x,y in test_ds.as_numpy_iterator():\n",
    "  targets.extend(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5n-nlHvkjhkL",
    "outputId": "1e72133d-5f25-4787-a3d8-a14e1d82cda6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(targets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnlUWwWmirtl",
    "outputId": "ef2fe08a-3047-41ed-d1e3-8ba918526b89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted_classes,np.argmax(targets,axis=-1))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pci1FMrJcuSs"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lPFztVMj3d6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
