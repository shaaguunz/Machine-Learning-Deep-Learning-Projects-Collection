{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs3Vxo5dmP7j"
   },
   "source": [
    "## Image classification NIN\n",
    "\n",
    "***NOTE***\n",
    "\n",
    "Be sure to use hardware acceleration to use the GPU. Click on `Runtime`, change `runtime type`, and select `GPU` for the *hardware accelerator* option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FUHcE89LT3x"
   },
   "source": [
    "* LeNet, AlexNet, and VGG all share a common design pattern: extract features exploiting spatial structure via a sequence of convolutions and pooling layers and post-process the representations via fully connected layers. The improvements upon LeNet by AlexNet and VGG mainly lie in how these later networks widen and deepen these two modules.\n",
    "\n",
    "* This design poses two major challenges. First, the fully connected layers at the end of the architecture consume tremendous numbers of parameters. This is a significant impediment to speedy computation, in particular on mobile and embedded devices.\n",
    "\n",
    "* Second, it is equally impossible to add fully connected layers earlier in the network to increase the degree of nonlinearity: doing so would destroy the spatial structure and require potentially even more memory.\n",
    "\n",
    "* The network in network (NiN) blocks of [Lin et al., 2013] offer an alternative, capable of solving both problems in one simple strategy. They were proposed based on a very simple insight: \n",
    "\n",
    "* (i) **use 1x1 convolutions** to add local nonlinearities across the channel activations and \n",
    "\n",
    "* (ii) **use global average pooling** to integrate across all locations in the last representation layer. \n",
    "\n",
    "* The significant difference between NiN and both AlexNet and VGG is that NiN avoids fully connected layers altogether. Instead, NiN uses a NiN block with a number of output channels equal to the number of label classes, followed by a global average pooling layer, yielding a vector of logits. This design significantly reduces the number of required model parameters, albeit at the expense of a potential increase in training time.\n",
    "\n",
    "* Global average pooling determines the average value across each channel. So if there are 128 channels in the input feature map, then the output will be 128 values.\n",
    "\n",
    "![From LeNet (left) to AlexNet (right).](https://d2l.ai/_images/nin.svg)\n",
    "\n",
    "(left AlexNet, right VGG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpvo-bqTmP7m"
   },
   "source": [
    "## Imports first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvCXvXq5mP7p",
    "outputId": "5700ecd4-d2d7-469b-e6ec-e4af87164028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqtYnFAUCAJN"
   },
   "source": [
    "## 1x1 convolution\n",
    "\n",
    "* The depth of the input or number of filters used in convolutional layers often increases with the depth of the network, resulting in an increase in the number of resulting feature maps. It is a common model design pattern.\n",
    "\n",
    "* A large number of feature maps in a convolutional neural network can cause a problem as a convolutional operation must be performed down through the depth of the input. This is a particular problem if the convolutional operation being performed is relatively large, such as 5×5 or 7×7 pixels, as it can result in considerably more parameters (weights) and, in turn, computation to perform the convolutional operations\n",
    "\n",
    "* Pooling layers are designed to downscale feature maps and systematically halve the width and height of feature maps in the network. Nevertheless, pooling layers do not change the number of filters in the model, the depth, or number of channels.\n",
    "\n",
    "* The solution is to use a 1×1 filter to down sample the depth or number of feature maps. A 1×1 filter will only have a single parameter or weight for each channel in the input, and like the application of any filter results in a single output value.\n",
    "\n",
    "* This structure allows the 1×1 filter to act like a single neuron with an input from the same position across each of the feature maps in the input. This single neuron can then be applied systematically with a stride of one, left-to-right and top-to-bottom without any need for padding, resulting in a feature map with the same width and height as the input.\n",
    "\n",
    "* The 1×1 filter is so simple that it does not involve any neighboring pixels in the input; it may not be considered a convolutional operation. Instead, it is a linear weighting or projection of the input. Further, a nonlinearity is used as with other convolutional layers, allowing the projection to perform non-trivial computation on the input feature maps.\n",
    "\n",
    "* This simple 1×1 filter provides a way to usefully summarize the input feature maps. The use of multiple 1×1 filters, in turn, allows the tuning of the number of summaries of the input feature maps to create, effectively allowing the depth of the feature maps to be increased or decreased as needed.\n",
    "\n",
    "A convolutional layer with a 1×1 filter can, therefore, be used at any point in a convolutional neural network to control the number of feature maps. As such, it is often referred to as a projection operation or projection layer, or even a feature map or channel pooling layer.\n",
    "\n",
    "In NiN, the depth of the 1x1 convolutionais is the same (i.e. the number of filters created is the same as in input feature map). So the depth is not reduced.\n",
    "\n",
    "### Next, let's see two examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWDehzoTB4as",
    "outputId": "40e95453-2450-4e2c-fc06-1fbdb5d078a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 256, 512)     14336     \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 512)     262656    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,992\n",
      "Trainable params: 276,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(512, (3,3), padding='same', activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(Conv2D(512, (1,1), activation='relu'))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6O9fTzzB4dk",
    "outputId": "dbb72b47-c51d-4782-c490-c1410b8f12c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 256, 256, 512)     14336     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 256, 256, 64)      32832     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,168\n",
      "Trainable params: 47,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(512, (3,3), padding='same', activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(Conv2D(64, (1,1), activation='relu'))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvLNnz0vDnHc"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1GTR35cmP7v"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQ_KjFJomP7x",
    "outputId": "30d58bcc-1120-493e-cd10-4b0ace0a4de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-Lw_bPQKQFX"
   },
   "source": [
    "## View the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueyWl-aUmP72",
    "outputId": "16ac3402-cb46-44ee-950c-805ad5a22f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape : ', X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bsoluyzBOYR",
    "outputId": "766cebe1-cf25-4d56-e251-4cb71f1c33f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape :  (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Testing data shape : ', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z51sdOntmP78"
   },
   "source": [
    "## Find the unique numbers from the train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o29O_gUTmP79",
    "outputId": "e36757b6-3a1d-43bd-f0a7-8fc1463cf023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  10\n",
      "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(Y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMMjLgbimP8B"
   },
   "source": [
    "## Plot some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "qQpW4rajmP8C",
    "outputId": "fb604ef5-d19c-4e6e-e495-ee9c45f5b635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground Truth : 1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_point = 15\n",
    "\n",
    "plt.figure(figsize=[10,5])\n",
    " \n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train[data_point,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(Y_train[data_point]))\n",
    " \n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_test[data_point,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(Y_test[data_point]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA_mGK_NVHwn"
   },
   "source": [
    "## Reshape needed\n",
    "\n",
    "Keras wants to know the depth of an image. \n",
    "\n",
    "For CNNS, Keras wants the format of the data as follows: [batches, width, height, depth]. \n",
    "\n",
    "In this case the colour channel/depth of the images is 1. Currently the shape is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBbn1qONB0M8",
    "outputId": "11723bb9-02ca-4975-cbec-79af97d5d085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmTJdq8LRDsx",
    "outputId": "324efa80-8452-45dc-ae75-3b2d3aa7c966"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a55ZpEbCB6kQ"
   },
   "source": [
    "But this doesn't have a depth value. So we can reshape it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8HTp2YHVH4_"
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvHP86fjRU8z",
    "outputId": "bbf27443-707e-4dc1-9c81-68fb06d109a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AMa4C0CRWb9",
    "outputId": "9699c87e-4af3-4c99-813a-3cb49e18aaf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lQP7CElWVfe"
   },
   "source": [
    "## View the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QKOvMPaWVfh",
    "outputId": "b94e59cc-da44-4fa9-ef8c-907cd563a2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 28, 28, 1) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape : ', X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzGxSUONCRb2",
    "outputId": "697bbdda-a372-419a-936a-ec1130273b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape :  (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Testing data shape : ', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfClFp-6K7P6"
   },
   "source": [
    "## Convert from categorical labels to one-hot encoded vectors\n",
    "\n",
    "In this case there are 10 classes so we can tell the function to convert into a vector of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3xjsPnVmP8j"
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7LVPnt1QPQM"
   },
   "source": [
    "## Small twist!\n",
    "\n",
    "API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQNnZHhNSEKW"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHvmE76ySEMx"
   },
   "outputs": [],
   "source": [
    "def resize_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    image = tf.image.resize(image, (224,224))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqEzyUPLSOdv"
   },
   "outputs": [],
   "source": [
    "train_ds = (train_ds\n",
    "                  .map(resize_images)\n",
    "                  .shuffle(buffer_size=10000)\n",
    "                  .batch(batch_size=64, drop_remainder=True))\n",
    "test_ds = (test_ds\n",
    "                  .map(resize_images)\n",
    "                  .batch(batch_size=10, drop_remainder=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VungsdeHmP8u"
   },
   "source": [
    "## Create a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gozmsPxhmP8v"
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # NiN block 1\n",
    "    # *********************************************\n",
    "    model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\n",
    "                                   activation='relu', input_shape=(224,224,1)))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2))\n",
    "\n",
    "    # NiN block 2\n",
    "    # *********************************************\n",
    "    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same',\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2))\n",
    "\n",
    "    # NiN block 3\n",
    "    # *********************************************\n",
    "    model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2))\n",
    "\n",
    "    # NiN block 4\n",
    "    # *********************************************\n",
    "    model.add(tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding='same',\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=10, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=10, kernel_size=1, strides=1,\n",
    "                                   activation='relu'))\n",
    "    model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q03EcUrxmP8x"
   },
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUN6K31smP80"
   },
   "source": [
    "## Determine the number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DxDanOHmP80",
    "outputId": "bcce5c70-4360-49f0-984d-c594e8220fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 54, 54, 96)        11712     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 54, 54, 96)        9312      \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 54, 54, 96)        9312      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 26, 26, 256)       614656    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 26, 26, 256)       65792     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 26, 26, 256)       65792     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 12, 12, 384)       885120    \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 12, 12, 384)       147840    \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 12, 12, 384)       147840    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 5, 384)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 5, 5, 10)          34570     \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 5, 5, 10)          110       \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 5, 5, 10)          110       \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 10)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10)                0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,992,166\n",
      "Trainable params: 1,992,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFE90QBiF66s"
   },
   "source": [
    "* NiN has dramatically fewer parameters than AlexNet and VGG. \n",
    "\n",
    "* This stems from the fact that it needs no giant fully connected layers and fewer convolutions with wide kernels. Instead, it uses local  convolutions and global average pooling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifPs0mftmP84"
   },
   "source": [
    "## Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8RZc1rhmP84",
    "outputId": "7c99610b-7d25-45ce-d383-b5267cefa678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 [==============================] - 90s 80ms/step - loss: 1.3859 - accuracy: 0.5367\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 75s 78ms/step - loss: 1.0207 - accuracy: 0.6554\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 75s 77ms/step - loss: 0.8446 - accuracy: 0.6856\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 75s 78ms/step - loss: 0.7926 - accuracy: 0.7092\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 75s 78ms/step - loss: 0.7521 - accuracy: 0.7196\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 75s 77ms/step - loss: 0.7276 - accuracy: 0.7277\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 74s 77ms/step - loss: 0.7055 - accuracy: 0.7344\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 74s 77ms/step - loss: 0.6903 - accuracy: 0.7392\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 74s 77ms/step - loss: 0.6762 - accuracy: 0.7434\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 74s 77ms/step - loss: 0.6591 - accuracy: 0.7485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3a05b89d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j9LecHgmP8_"
   },
   "source": [
    "## Predict on all the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7TtfkDWjmP8_",
    "outputId": "0f7af5e1-5e1a-4938-e9f4-27b93b38185c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b_zM7W80cFhJ",
    "outputId": "e86357e2-85d0-4b9f-d729-9d6ac0f194bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0abcDBFmcCz5"
   },
   "outputs": [],
   "source": [
    "correct_values = np.argmax(Y_test,axis=-1)\n",
    "predicted_classes = np.argmax(predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "N7keRohxmP9H",
    "outputId": "769089ad-66da-4391-a8b8-5602d442f075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.87"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted_classes,correct_values)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1Ik7Odwj9L6"
   },
   "source": [
    "### More efficient (or rather, less RAM intensive) way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "agkf9IKkiotI"
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "for x,y in test_ds.as_numpy_iterator():\n",
    "  targets.extend(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5n-nlHvkjhkL",
    "outputId": "73c0f07c-05d3-47a1-dda2-1d962726431a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(targets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AnlUWwWmirtl",
    "outputId": "daa16e40-951d-42f6-fba7-6ad5b1218f38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.87"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted_classes,np.argmax(targets,axis=-1))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pci1FMrJcuSs"
   },
   "source": [
    "## Tasks:\n",
    "\n",
    "* Simplify the network so that it roughly takes less than 40 seconds to run 1 epoch. Your network should contain 1x1 convolutions. You can modify hyper-parameters\n",
    "\n",
    "* Who got the best testing performance with the least number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6lPFztVMj3d6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
